{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # таблицы\n",
    "\n",
    "# токенизация\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "download(\"punkt\")\n",
    "download(\"stopwords\")\n",
    "\n",
    "from textblob import TextBlob # сентимент-анализ\n",
    "\n",
    "dir = \"D:\\HSE\\Pump_Dump\\\\\"  # папка проекта\n",
    "\n",
    "# список символов для удаления\n",
    "with open(dir + \"clear_text.txt\", encoding=\"utf8\") as tmp_txt:\n",
    "    clear_sym = tmp_txt.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЭТОТ КОД ЛУЧШЕ ПЕРЕМЕСТИТЬ В КЛАСС \n",
    "# загрузка фрейма и создание списка тикеров с альтернативными названиями\n",
    "tickers_df = pd.read_csv(\n",
    "    dir + \"market\\\\moex_my.csv\", encoding=\"utf-8\", sep=\"|\", keep_default_na=False\n",
    ")\n",
    "tickers = list()\n",
    "# в каждой строке собираем значения всех столбцов\n",
    "for i in range(0, len(tickers_df)):\n",
    "    tickers_temp = list()\n",
    "    tickers_temp += tickers_df.iloc[i][\"FULL_NAME\"].split(\",\")\n",
    "    tickers_temp += tickers_df.iloc[i][\"SHORT_NAME\"].split(\",\")\n",
    "    tickers_temp += tickers_df.iloc[i][\"NORM_NAME\"].split(\",\")\n",
    "    tickers_temp += tickers_df.iloc[i][\"MORPH_NAMES\"].split(\",\")\n",
    "    tickers_temp += tickers_df.iloc[i][\"MY_NAMES\"].split(\",\")\n",
    "    # добавляем в сет для удаления дублей и фильтруем для удаления пустых значений\n",
    "    tickers_temp = set(tickers_temp)\n",
    "    tickers_temp = list(filter(None, tickers_temp))\n",
    "    # тикеры ставим в начало списка\n",
    "    tickers_temp = tickers_df.iloc[i][\"TICKERS\"].split(\",\") + tickers_temp\n",
    "    tickers.append(tickers_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизация. Параметр - любой текст. Возвращает список токенов текста\n",
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    text_without_punkt = []\n",
    "    for word in text:\n",
    "        if word[0].isalpha():\n",
    "            text_without_punkt.append(word)\n",
    "    # в стоп-словах stop-words NLTK есть кейсы, которые могут влиять на восприятие текста (не исключено -> исключено)\n",
    "    stop_words = stopwords.words(\"russian\")\n",
    "    # добавляем свои стоп-слова из файла clear_text.txt\n",
    "    stop_words = stop_words + clear_sym\n",
    "    clean_text = []\n",
    "    for word in text_without_punkt:\n",
    "        if word not in stop_words:\n",
    "            clean_text.append(word)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тикеризация. Параметр - список токенов текста. Возвращает список тикеров текста\n",
    "def tickers_find(tokens_list):\n",
    "    # список токенов преобразуем в set для удаления дублей токенов\n",
    "    tokens_list = set(tokens_list)\n",
    "    tickers_list = set()\n",
    "    temp_text = \"\"\n",
    "    for token in tokens_list:\n",
    "        # ищем в сете с тикерами и альтернативными названиями TICKERS|SHORT_NAME|NORM_NAME|MORPH_NAMES|MY_NAMES\n",
    "        find_index = list(tickers_df[tickers_df.isin([token]).any(axis=1)].index.values)\n",
    "        if len(find_index) > 0:\n",
    "            temp_text = (\n",
    "                tickers_df.loc[[int(find_index[0])]][\"TICKERS\"].values[0].split(\",\")[0]\n",
    "            )\n",
    "        # если по единичным значениям столбцов не найдено, то берем полный список альтернативных названий\n",
    "        else:\n",
    "            for ticker in tickers:\n",
    "                if token in ticker:\n",
    "                    temp_text = ticker[0]\n",
    "        if len(temp_text) > 0:\n",
    "            tickers_list.add(temp_text)\n",
    "    # print (tokens_list,list(tickers_list),sep=\"\\n\")\n",
    "    return list(tickers_list)\n",
    "\n",
    "# для тестов по произвольным строкам\n",
    "# print(tickers_find([\"цифры\",\"сургутнефтегаз\",\"irao\",\"лукойл\",]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считывание и нормализация сырых данных канала. Параметр - название ТГ-канала (загруженный на диск CSV). Возвращает  фрейм с данными канала\n",
    "def channel_post(channel):\n",
    "    channel_path = dir + \"TG\\\\\" + channel + \".csv\"\n",
    "    # загрузка файла\n",
    "    df = pd.read_csv(\n",
    "        channel_path,\n",
    "        encoding=\"utf-8\",\n",
    "        sep=\"|\",\n",
    "        keep_default_na=False,\n",
    "    )\n",
    "    # преобразуем поле с датой новости в тип ДАТА-ВРЕМЯ\n",
    "    df[\"NEWS_DATE\"] = pd.to_datetime(df[\"NEWS_DATE\"], dayfirst=True, errors=\"ignore\")\n",
    "    # создаем поля для статистики по датам Номер недели\n",
    "    df = df.assign(NEWS_YEAR=df[\"NEWS_DATE\"].dt.isocalendar().year)\n",
    "    df = df.assign(NEWS_WEEK=df[\"NEWS_DATE\"].dt.isocalendar().week)\n",
    "    # создаем поля для ключевых слов, тикеров и сентимента\n",
    "    df = df.assign(WORDS=\"\")\n",
    "    df = df.assign(TICKERS=\"\")\n",
    "    df = df.assign(MOOD=\"\")\n",
    "    # в некоторых новостях встречаются дубли текстов, удаляем их\n",
    "    df.drop_duplicates(subset=\"NEWS_TEXT\", keep=\"first\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сентимент анализ\n",
    "def mood_text(text):\n",
    "    # Используем TextBlob для анализа тональности\n",
    "    analysis = TextBlob(text)\n",
    "    # Оцениваем настроение текста\n",
    "    sentiment = analysis.sentiment.polarity\n",
    "    # The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity).\n",
    "    # The polarity score is a float within the range [-1.0, 1.0].\n",
    "    # The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "    # print(analysis.sentiment)\n",
    "    if sentiment > 0:\n",
    "        sentiment_label = \"positive\"\n",
    "    elif sentiment < 0:\n",
    "        sentiment_label = \"negative\"\n",
    "    else:\n",
    "        sentiment_label = \"neutral\"\n",
    "\n",
    "    return sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Будет обработано:  30\n",
      "1 signals_moex\n",
      "2 sonbuffet\n",
      "3 smfanton\n",
      "4 investoksana\n",
      "5 kapitalopedia\n",
      "6 russia_companies\n",
      "7 dohod_investment\n",
      "8 sgcapital\n",
      "9 ruforecasts\n",
      "10 anoniminvest\n",
      "11 iporobinhood\n",
      "12 mamonova_finance\n",
      "13 dohod\n",
      "14 if_stocks\n",
      "15 lemonfortea\n",
      "16 div_invest\n",
      "17 tinkoff_invest_official\n",
      "18 finamalert\n",
      "19 bcs_express\n",
      "20 omyinvestments\n",
      "21 alfa_investments\n",
      "22 fm_invest\n",
      "23 cbrstocks\n",
      "24 AK47pfl\n",
      "25 fineconomics\n",
      "26 invest_or_lost\n",
      "27 profitking\n",
      "28 bizlike\n",
      "29 spydell_finance\n",
      "30 divForever\n"
     ]
    }
   ],
   "source": [
    "# тикеризация и токенизация текста\n",
    "channels = pd.read_csv(dir + \"TG\\\\\" + \"!channels.csv\", delimiter=\"|\")\n",
    "print(\"Будет обработано: \",len(channels))\n",
    "k=1\n",
    "# обрабатываем каждый канал\n",
    "for channel in channels[\"CHANNEL\"].values:\n",
    "    print(k, channel)\n",
    "    df = channel_post(channel)\n",
    "    # для каждой новости создаем список ключевых слов и выделяем тикеры\n",
    "    for i in range(0, len(df)):\n",
    "        news_text = df.iloc[i][\"NEWS_TEXT\"]\n",
    "        words_text = clear_text(news_text)\n",
    "        df[\"WORDS\"].values[i] = \",\".join(words_text)\n",
    "        df[\"TICKERS\"].values[i] = \",\".join(tickers_find(words_text))\n",
    "        df[\"MOOD\"].values[i] = mood_text(news_text)\n",
    "\n",
    "    # обновленный фрейм записываем на диск с признаком _post (пост-обработка)\n",
    "    df.to_csv(\n",
    "        dir + \"TG\\\\\" + channel + \"_post.csv\",\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "        sep=\"|\",\n",
    "    )\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для тестов тикеризации\n",
    "# txt = \"Возможно, «Газпром» выплатит дивиденды за 2023 год, не исключено, что с дивдоходностью около 10%. Но это все равно ниже безрисковой ставки и того же «Сбера», у которого бизнес более привлекательный – Алор Брокер У цен на газ предпосылок для роста пока нет, скорее, консолидация в районе $300 за 1 тыс. куб. м. Это умеренно негативно для «Газпрома», так как компания зависит и от спроса Европы на газ, и от цен. Мы считаем, что Китай не сможет в полной мере покрыть выпавшие европейские объемы. По акциям «Газпрома» сохраняем рекомендацию «держать» и ориентир 190 рублей за акцию – ПСБ При отсутствии признаков дальнейшего ускорения роста цен до конца января не исключаем возврат кривой ОФЗ к ноябрьским минимумам по доходности 11,6% по 5 летним ОФЗ и 11,7 11,75% по 10 летним – ПСБ Шансов на выход рынка из боковика вверх больше, чем на уход индекса МосБиржи ниже 3100 пунктов. В такой ситуации инвесторам стоит занимать выжидательную позицию, а спекулянтам – искать идеи во втором третьем эшелоне – Алор Брокер Причин для девальвации российской валюты в ближайшее время мы не видим. Рубль по прежнему будет получать поддержку от огромных продаж валюты Банком России и низкого спроса импортеров на доллары и юани в начале года эшелоне – Алор Брокер Finam Alert это рыночные сигналы, идеи, торговые прогнозы|2024|3|возможно, газпром, выплатит, дивиденды, год, исключено, дивдоходностью, около, это, равно, ниже, безрисковой, ставки, сбера, которого, бизнес, привлекательный, алор, брокер, цен, газ, предпосылок, роста, пока, скорее, консолидация, районе, тыс, куб, м., это, умеренно, негативно, газпрома, компания, зависит, спроса, европы, газ, цен, считаем, китай, сможет, полной, мере, покрыть, выпавшие, европейские, объемы, акциям, газпрома, сохраняем, рекомендацию, держать, ориентир, рублей, акцию, псб, отсутствии, признаков, дальнейшего, ускорения, роста, цен, конца, января, исключаем, возврат, кривой, офз, ноябрьским, минимумам, доходности, летним, офз, летним, псб, шансов, выход, рынка, боковика, вверх, уход, индекса, мосбиржи, ниже, пунктов, ситуации, инвесторам, стоит, занимать, выжидательную, позицию, спекулянтам, искать, идеи, втором, третьем, эшелоне, алор, брокер, причин, девальвации, российской, валюты, ближайшее, время, видим, рубль, прежнему, получать, поддержку, огромных, продаж, валюты, банком, россии, низкого, спроса, импортеров, доллары, юани, начале, года, эшелоне, алор, брокер, finam, alert, это, рыночные, сигналы, идеи, торговые, прогнозы\"\n",
    "# print(clear_text(txt))\n",
    "# print(tickers_find(clear_text(txt)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
